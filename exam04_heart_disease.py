# -*- coding: utf-8 -*-
"""exam04_heart_disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ANaBURHUkMsdTScm9ynxFmNBLdPgECl
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

"""1. age	나이 (int)
2. sex	성별 (1, 0 / int)
3. chest pain type (4 values)	가슴 통증 타입 (0 ~ 3 / int)
4. resting blood pressure	혈압
5. serum cholestoral in mg/dl	혈청 콜레스테롤
6. fasting blood sugar > 120 mg/dl	공복 혈당
7. resting electrocardiographic results	심전도
8. maximum heart rate achieved	최대 심장박동 수
9. exercise induced angina	운동 유도 협심증 (이게 뭐죠?)
10. oldpeak = ST depression induced by exercise relative to rest	노약 =운동에 의해 유발되는 St 우울증 
11. the slope of the peak exercise ST segment	ST 세그먼트의 기울기
12. number of major vessels (0-3) colored by flourosopy	혈관의수
13. thal : 3 = normal; 6 = fixed defect; 7 = reversable defect	
"""

column_name = ['age','sex','cp','treshbps','chol','fbs','restecg','thalach','exang','oldpeak','slop','ca','thal','HeartDisease'] #컬럼의 내용
raw_data = pd.read_excel('./data/heart-disease.xlsx',header=None, names=column_name) #pandas 엑셀을 불러옴
print(raw_data.head())

raw_data.info()

raw_data.describe()

clean_data = raw_data.replace('?',np.nan) #?를 not a number로 바꿈
clean_data = clean_data.dropna() #nan을 날림
clean_data.info()

keep = column_name.pop() #심장병을 뺴냄
print(keep)
print(column_name)

training_data = clean_data[column_name] #심장병 뺀 데이터
target =clean_data[[keep]] #심장병 데이터
print(training_data)
print(target)

print(target['HeartDisease'].sum()) #심장병 걸린사람 수

print(target['HeartDisease'].mean()) #심장병 걸린사람의 비율

from sklearn.preprocessing import StandardScaler #3가지 방법으로 스케일가능
scaler = StandardScaler()
scaled_data = scaler.fit_transform(training_data)
scaled_data = pd.DataFrame(scaled_data,columns=column_name)
print(scaled_data)

scaled_data.describe().T

boxplot = scaled_data.boxplot(column=column_name,showmeans=True)
plt.show()

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(scaled_data,target,test_size=0.30)#교차검증 traintestsplit 랜덤하게
print('x_train shape',x_train.shape)
print('y_train_shape',y_train.shape)
print('x_test shape',x_test.shape)
print('y_test shape',y_test.shape)

model = Sequential()
model.add(Dense(512,input_dim=13,activation='relu'))
model.add(Dropout(0.25)) #dropout random 일부를 떼어내는 것 - 과적합을 하지 않기위함
model.add(Dense(256,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(128,activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(1,activation='sigmoid')) #이진분류
model.summary()

model.compile(loss='mse',optimizer='adam',metrics=['binary_accuracy'])
fit_hist = model.fit(x_train,y_train, batch_size=50,epochs= 20, validation_split=0.2, verbose=1)#validation 복원추출
#batch_size 학습의 폭을 끊음 시간이 늘어나지면 메모리부족시 사용가능

plt.plot(fit_hist.history['binary_accuracy'])
plt.plot(fit_hist.history['val_binary_accuracy'])
plt.show()

score = model.evaluate(x_test,y_test,verbose=0) #evaluate - forward mse까지만 구함
print('Keras DNN model loss :',score[0])
print('Keras DNN model accuracy:',score[1])